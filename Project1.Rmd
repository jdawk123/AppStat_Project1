---
title: "Project1_EricLaigaie"
author: "Eric Laigaie"
date: "1/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
options(scipen=999)
library(tidyverse)
library(readr)
library(fastDummies)
library(car)
library(olsrr)
library(MASS)
library(qpcR)
library(class)
library(caret)
library(rpart)
library(rpart.plot)
library(e1071)
library(stringr)
```

```{r}
data <- read_csv("https://raw.githubusercontent.com/ericlaigaie/AppStat_Project1/main/data1.csv")
```
```{r}
colSums(is.na(data))
```

```{r}
# Engine Fuel Type Missing Values - All 2004 Suzuki Veronas
#data %>% filter(is.na(`Engine Fuel Type`))

# All Suzuki Veronas and all 2004 Suzukis are 'regular unleaded' - Will input that value
data <- data %>%
  mutate(`Engine Fuel Type` = ifelse(is.na(`Engine Fuel Type`), 'regular unleaded', `Engine Fuel Type`))
```

```{r}
# Engine HP Missing Values - ~.57% of data
#data %>% filter(is.na(`Engine HP`))

# We have 69 missing values spread across ~13 different models
# Some of these models have data from other years, while some will require more research

# Chevy Impala - Flex-fuel
#data %>% filter(Make == 'Chevrolet' & Model == 'Impala') %>% dplyr::select(Year, `Engine Fuel Type`, `Engine HP`)
# All other flex-fuel models have 305 HP
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Impala', 305, `Engine HP`))

# Ford Escapes
#data %>% filter(Model == 'Escape') %>% dplyr::select(Year, `Engine Fuel Type`, `Engine HP`)
# All other regular unleaded models have 168 horsepower
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Escape', 168, `Engine HP`))

# Freestars
#data %>% filter(Model == 'Freestar')
# 2006 models are split 193/201 and 2007 models are all 2001. According to autoblog.com, the 2005 Freestar has 193 hp.
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Freestar', 193, `Engine HP`))

# i-MiEV
#data %>% filter(Model == 'i-MiEV')
# Both other i-MiEVs have 66 hp. This matches with research from edmunds.com
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'i-MiEV', 66, `Engine HP`))

# M-Class
#data %>% filter(Model == 'M-Class')
# All other diesel models have 240 horsepower.
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'M-Class', 240, `Engine HP`))

# MKZ
#data %>% filter(Model == 'MKZ')
# All other MKZ with Luxury,Hybrid Market Categories have 188 HP.
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'MKZ', 188, `Engine HP`))

# RAV4 EV
#data %>% filter(Model == 'RAV4 EV')
# All other RAV4 EV models have 154 horsepower. This matches with research from edmunds.com
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'RAV4 EV', 154, `Engine HP`))

# Fiat 500e - 111 horsepower (kbb.com)
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == '500e', 111, `Engine HP`))
# Focus - 143 horsepower (MotorTrend.com)
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Focus', 143, `Engine HP`))
# Fit EV - 123 horsepower (car & driver)
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Fit EV', 123, `Engine HP`))
# Soul EV - 109 horsepower (car & driver)
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Soul EV', 109, `Engine HP`))
# Continental - 335 horsepower (carconnection.com)
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Continental', 335, `Engine HP`))
# Leaf - 107 horsepower (USNews)
data <- data %>% mutate(`Engine HP` = ifelse(is.na(`Engine HP`) & Model == 'Leaf', 107, `Engine HP`))

# All other missing values were fixed with additional research. For instance, the Tesla Model S missing values had varying MSRP's, so trim levels had to be researched to find specific HP values. These remaining null values have already been imputed into the dataset before loading in.
```

```{r}
# Engine Cylinders Missing Values
#data %>% filter(is.na(`Engine Cylinders`))

# All electric cars have 0 cylinders
data <- data %>% mutate(`Engine Cylinders` = ifelse(`Engine Fuel Type` == 'electric', 0, `Engine Cylinders`))

# Google says that all the remaining cars (RX-7 '93-'95, RX-8 '09-'11) have 2 cylinders
data <- data %>% mutate(`Engine Cylinders` = ifelse(is.na(`Engine Cylinders`), 2, `Engine Cylinders`))
```

```{r}
# Number of Doors Missing Values
#data %>% filter(is.na(`Number of Doors`))

# Only two models here - '16 Tesla Model S (4 doors) & '13 Ferrari FF (2 doors)
# This was checked with other FF's and S's within the dataset.
data <- data %>%
  mutate(`Number of Doors` = ifelse(is.na(`Number of Doors`),
                                    ifelse(Model == 'FF', 2 ,4), `Number of Doors`))
```

```{r}
# UNKNOWN Transmission Types
#data %>% filter(`Transmission Type` == 'UNKNOWN')

# 5 models...

#data %>% filter(Make == 'Oldsmobile' & Model == 'Achieva') # MANUAL ---- look at Engine Cylinders
data <- data %>% mutate(`Transmission Type` = 
                          ifelse(`Transmission Type`=='UNKNOWN' & Model == 'Achieva', 'MANUAL', `Transmission Type`))

#data %>% filter(Make == 'Pontiac' & Model == 'Firebird') # MANUAL ---- look at Engine Fuel Type
data <- data %>% mutate(`Transmission Type` = 
                          ifelse(`Transmission Type`=='UNKNOWN' & Model == 'Firebird', 'MANUAL', `Transmission Type`))

#data %>% filter(Make == 'GMC' & Model == 'Jimmy') # MANUAL ---- all manuals are compact, autos are midsize
data <- data %>% mutate(`Transmission Type` = 
                          ifelse(`Transmission Type`=='UNKNOWN' & Model == 'Jimmy', 'MANUAL', `Transmission Type`))

#data %>% filter(Make == 'Chrysler' & Model == 'Le Baron') # AUTOMATIC
data <- data %>% mutate(`Transmission Type` = 
                          ifelse(`Transmission Type`=='UNKNOWN' & Model == 'Le Baron', 'AUTOMATIC', `Transmission Type`))

#data %>% filter(Make == 'Dodge' & Model == 'RAM 150') # MANUAL
data <- data %>% mutate(`Transmission Type` = 
                          ifelse(`Transmission Type`=='UNKNOWN' & Model == 'RAM 150', 'MANUAL', `Transmission Type`))
```

```{r}
# Last check for Nulls
anyNA(data)
```

```{r}
# ID Column Creation
data$ID <- 1:nrow(data)

# In the next few cells, we will be splitting up data and assigning new variables. To successfully add these new variables to 'data', we must preserve an id column.
```

```{r}
# Market Category consolidation ------------------------------------------------------------------------------------------

# Target Categories: Exotic, Luxury, Performance, High-Performance
# Other categories: Flex Fuel, Hatchback, Crossover, Factory Tuner, Diesel, Hybrid

# We won't be able to impute the pure Market Categories. So, how can we keep some of this information?
# Exotic/Luxury/Performance stand out to me as the most important categories
# The other categories should have their information accounted for in other variables (Engine Fuel Type, Vehicle Style)

# Below are code cells exploring **EXOTIC**, **LUXURY**, AND **PERFORMANCE/HIGH-PERFORMANCE**.
# In short:
#       Naive-Bayes can predict **EXOTIC** will good accuracy, so I've used that for now.
#       No ML models can classify **LUXURY**, so I used Make-specific approaches to fill in the N/As.
#       **PERFORMANCE** appears too obscure to classify with models or Make-specific approaches.
```

```{r}
# Exotic Exploration -----------------------------------------------------------------------------------------------------
e <- data %>% filter(str_detect(`Market Category`, 'Exotic'))
ne <- data %>% filter(str_detect(`Market Category`, 'Exotic') == FALSE & `Market Category` != 'N/A')
e$Exotic = 1
ne$Exotic = 0
ene <- rbind(e, ne)

#ggplot(ene, aes(x=`Engine HP`, y=as.factor(Exotic), fill=as.factor(Exotic))) + geom_boxplot()
#ggplot(ene, aes(x=MSRP, y=as.factor(Exotic), fill=as.factor(Exotic))) + geom_boxplot()
#ggplot(ene, aes(x=MSRP, y=`Engine HP`, color=as.factor(Exotic))) + geom_point() + xlim(0, 750000) + ylim(0, 800)

# Train / Test Split for Classifier Models -------------------------------------------------------------------------------
idx <- sample(seq(1, 2), size = nrow(ene), replace = TRUE, prob = c(.8, .2))
ene_train <- ene[idx == 1,]
ene_test <- ene[idx == 2,]

# Knn --------------------------------------------------------------------------------------------------------------------
#classifications = knn(ene_train[,c(5,16)],ene_test[,c(5,16)],ene_train$Exotic, prob = TRUE, k = 5)
#confusionMatrix(table(classifications,ene_test$Exotic))
# ACC = 96.8 / SPE = 98.8 / SEN = 70.3 

# Naive-Bayes ------------------------------------------------------------------------------------------------------------
model = naiveBayes(ene_train[,c(5, 16)],ene_train$Exotic,laplace = 1)
confusionMatrix(table(predict(model,ene_test[,c(5,16)]),ene_test$Exotic))
# ACC = 96.3 / SPE = 96.9 / SEN = 87.3

# Logistic Regression ----------------------------------------------------------------------------------------------------
#log_reg <- glm(Exotic ~ `Engine HP` + MSRP,family=binomial(link='logit'),data=ene_train)
#fitted.results <- predict(log_reg,newdata=ene_test,type='response')
#confusionMatrix(table(ifelse(fitted.results > 0.5,1,0),ene_test$Exotic))
# ACC = 97.5 / SPE = 99.5 / SEN = 71.2

# Decision Tree ----------------------------------------------------------------------------------------------------------
#fit <- rpart(Exotic ~ `Engine HP` + MSRP, data = ene_train, method = 'class')
#confusionMatrix(table(predict(fit, ene_test, type = 'class'), ene_test$Exotic))
# ACC = 98.1 / SPE = 99.3 / SEN = 82.2

# ********************************************************************************************************************** #
# Naive-Bayes looks like a solid option. Definitely usable if we want to go that route
# ********************************************************************************************************************** #

# Code to implement Exotic changes ---------------------------------------------------------------------------------------
na_Exotic <- data %>% filter(`Market Category` == 'N/A')
na_Exotic$Exotic <- predict(model, na_Exotic[,c(5,16)])
total_Exotic <- rbind(ene, na_Exotic)
total_Exotic <- total_Exotic %>% dplyr::select(ID, Exotic)
data <- merge(data, total_Exotic, by='ID')
data$Exotic <- as.numeric(data$Exotic)
```

```{r}
# Luxury Exploration -----------------------------------------------------------------------------------------------------
l <- data %>% filter(str_detect(`Market Category`, 'Luxury'))
nl <- data %>% filter(str_detect(`Market Category`, 'Luxury') == FALSE & `Market Category` != 'N/A')
l$Luxury = 1
nl$Luxury = 0
lnl <- rbind(l, nl)

# Train / Test Split for Classifier Models -------------------------------------------------------------------------------
#idx <- sample(seq(1, 2), size = nrow(lnl), replace = TRUE, prob = c(.8, .2))
#lnl_train <- lnl[idx == 1,]
#lnl_test <- lnl[idx == 2,]

# Knn --------------------------------------------------------------------------------------------------------------------
#classifications = knn(lnl_train[,c(5,16)],lnl_test[,c(5,16)],lnl_train$Luxury, prob = TRUE, k = 5)
#confusionMatrix(table(classifications,lnl_test$Luxury))
# ACC = 78.7 / SPE = 81.0 / SEN = 75.2

# Naive-Bayes ------------------------------------------------------------------------------------------------------------
#model = naiveBayes(lnl_train[,c(5,16)],lnl_train$Luxury,laplace = 1)
#confusionMatrix(table(predict(model,lnl_test[,c(5,16)]),lnl_test$Luxury))
# ACC = 62.4 / SPE = 86.9 / SEN = 26.0

# Logistic Regression ----------------------------------------------------------------------------------------------------
#log_reg <- glm(Luxury ~ `Engine HP` + MSRP,family=binomial(link='logit'),data=lnl_train)
#fitted.results <- predict(log_reg,newdata=lnl_test,type='response')
#confusionMatrix(table(ifelse(fitted.results > 0.5,1,0),lnl_test$Luxury))
# ACC = 61.2 / SPE = 85.9 / SEN = 24.5

# Decision Tree ----------------------------------------------------------------------------------------------------------
#fit <- rpart(Luxury ~ `Engine HP` + MSRP, data = lnl_train, method = 'class')
#confusionMatrix(table(predict(fit, lnl_test, type = 'class'), lnl_test$Luxury))
# ACC = 82.3 / SPE = 84.7 / SEN = 78.9

# ********************************************************************************************************************** #
# Decision Tree is the best predictor, but I'm still not happy with the results. We may be able to up that accuracy by
# changing parameters, but a more tedious solution may be better (have luxury and non-luxury Makes with a MSRP threshold)
# ********************************************************************************************************************** #

# Further Luxury Exploration ---------------------------------------------------------------------------------------------
'%!in%' <- function(x,y)!('%in%'(x,y))

# Makes that only produce Luxury Cars
lux_df <- data %>% filter(Make %in% l$Make & Make %!in% nl$Make) %>% group_by(Make) %>% summarize(n=n())
lux_df$Extra <- numeric(nrow(lux_df))
# Finding if there are 'N/A' Market Categories for cars within these Luxury Makes
for(i in 1:nrow(lux_df)) {
    row <- lux_df[i,]
    lux_df$Extra[i] <- nrow(data%>%filter(Make==row$Make & `Market Category` == 'N/A'))
    
}
#lux_df
# ********************************************************************************************************************** #
# There ARE NOT extra Luxury-make cars to classify
# ********************************************************************************************************************** #


# Non-Luxury Makes
nolux_df <- data %>% filter(Make %in% nl$Make & Make %!in% l$Make) %>% group_by(Make) %>% summarize(n=n())
nolux_df$Extra <- numeric(nrow(nolux_df))
# Finding if there are 'N/A' Market Categories for cars within these Luxury Makes
for(i in 1:nrow(nolux_df)) {
    row <- nolux_df[i,]
    nolux_df$Extra[i] <- nrow(data%>%filter(Make==row$Make & `Market Category` == 'N/A'))
    
}
#nolux_df
# ********************************************************************************************************************** #
# There ARE extra Non-Luxury-make cars to classify
# ********************************************************************************************************************** #

# Luxury & Non-Luxury Makes
nln_df <- data %>% filter(Make %in% l$Make & Make %in% nl$Make) %>% group_by(Make) %>% summarize(n=n())
nln_df$Extra <- numeric(nrow(nln_df))
# Finding if there are 'N/A' Market Categories for cars within these Luxury Makes
for(i in 1:nrow(nln_df)) {
    row <- nln_df[i,]
    nln_df$Extra[i] <- nrow(data%>%filter(Make==row$Make & `Market Category` == 'N/A'))
    
}
#nln_df
# ********************************************************************************************************************** #
# There ARE extra Combo-Luxury-make cars to classify
# ********************************************************************************************************************** #


# Numbers check -- All good ----------------------------------------------------------------------------------------------
#sum(lux_df$Extra) + sum(nolux_df$Extra) + sum(nln_df$Extra)

# How do cars from non-luxury Makes with 'N/A' Mark-Cat compare in MSRP to their fellow models with Luxury=0?-------------
na_lux <- data %>% filter(`Market Category` == 'N/A')
na_lux$Luxury = 2
nanolux <- rbind(na_lux, lnl)
nanolux %>% filter(Make %in% nolux_df$Make) %>%
  ggplot(aes(x=MSRP, y=as.factor(Make), color=as.factor(Luxury))) + geom_point() + xlim(0, 750000) + facet_wrap(~Luxury)

# ********************************************************************************************************************** #
# ALL Non-Luxury-Make cars can be defined as Non-Luxury
# ********************************************************************************************************************** #


# How do cars from Lux&Non-Lux Makes with 'N/A' Mark-Cat compare in MSRP?-------------------------------------------------
nanolux %>% filter(Make %in% nln_df$Make) %>%
  ggplot(aes(x=MSRP, y=as.factor(Luxury), color=as.factor(Luxury))) + geom_point() + xlim(0, 150000) + facet_wrap(~Make)

# Many of these seem pretty simple -- classify non-luxury -- Further research into Buick/Chevy/Chrysler/GMC---------------

#nanolux %>% filter(Make == 'Buick' & MSRP < 20000) # Reatta's are Luxury, all others are Non-Luxury

#nanolux %>% filter(Make == 'Chevrolet' & MSRP > 50000 & MSRP < 60000) # Only Luxury's are Tahoe Hybrids from 2013
# Would feel comfortable making all N/A non-luxury

#nanolux %>% filter(Make == 'Chrysler' & MSRP < 20000) # Similar to Chevy - only 1991 TC's are Luxury
# Would feel comfortable making all N/A non-luxury

#nanolux %>% filter(Make == 'GMC' & Luxury == 1) # Similar to Chevy - only <2013 Yukon's are Luxury
# Would feel comfortable making all N/A non-luxury

# ********************************************************************************************************************** #
# For Combo-Luxury-Make cars, Hyundai/Kia/Toyota/VW cars are non-luxury. Other Makes have specific rules listed above.
# ********************************************************************************************************************** #

# Code to implement Luxury changes----------------------------------------------------------------------------------------
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make %in% nolux_df$Make, 0, Luxury))
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make == 'Buick' & Model == 'Reatta', 1, Luxury))
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make == 'Buick', 0, Luxury))
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make == 'Chevrolet', 0, Luxury))
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make == 'Chrysler', 0, Luxury))
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make == 'GMC', 0, Luxury))
nanolux <- nanolux %>% mutate(Luxury = ifelse(Luxury == 2 & Make %in% c('Hyundai','Kia','Toyota','Volkswagen'),0,Luxury))

nanolux <- nanolux %>% dplyr::select(Luxury, ID)
data <- merge(data, nanolux, by='ID')
```

```{r}
# Quick Numbers & Optics Check (Should be 8332, 3081, 292, 209)
data %>% group_by(Exotic, Luxury) %>% summarize(n=n())
ggplot(data, aes(x=MSRP, y=`Engine HP`, color=as.factor(Exotic))) + geom_point() + xlim(0, 750000) + ylim(0, 800)
ggplot(data, aes(x=MSRP, y=`Engine HP`, color=as.factor(Luxury))) + geom_point() + xlim(0, 750000) + ylim(0, 800)
```

```{r}
# Correlation Matrix
library(corrplot)

data_num <- dplyr::select_if(data, is.numeric)

res <- cor(data_num)
round(res, 2)

corrplot(res, method='number', type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

# Remove city mpg & Engine Cylinders
drops <- c("city mpg","Engine Cylinders")
data <- data[ , !(names(data) %in% drops)]
```

```{r}
# Summary Stats Table
library(vtable)
st(data_num)
```

```{r}
# MSRP and Popularity
cor(data$MSRP, data$Popularity)

ggplot(data, aes(x=MSRP, y=Popularity)) + 
  geom_point(color='black') + 
  labs(x='Retail Price (MSRP)', title='MSRP vs. Popularity')

# Really seems like there's nothing overarching here. The Popularity variable needs to be remade (we can talk about this in # the paper)
```

```{r}
# Switch 'Year' to 'Age' --------------------------------------------------------------------------------------------------
data$Age <- 2022 - data$Year

# 80/20 Train/Test Split for Objective 1 ----------------------------------------------------------------------------------
idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(.8, .2))
train <- data[idx == 1,]
test <- data[idx == 2,]

# Simple, handmade regression model ---------------------------------------------------------------------------------------
model <- lm(log(MSRP) ~ 
              Age + 
              `Engine HP` + 
              `highway MPG` + 
              Popularity + 
              Exotic + 
              Luxury, data=train)
summary(model)
par(mfrow=c(2,2))
plot(model)

ggplot(train, aes(x=model$residuals)) + geom_histogram(fill='red', color='black') + stat_function(fun = dnorm, args = list(mean = mean(model$residuals), sd = sd(model$residuals))) + labs(title='Residual Histogram')

test$MSRP_predictions <- predict(model, newdata=test)
test$MSRP_predictions <- exp(test$MSRP_predictions)

test$Diff <- abs(test$MSRP-test$MSRP_predictions)

AvgDiff <- round(mean(test$Diff),2)
AvgDiffPerc <- round(100*(AvgDiff / mean(test$MSRP)),2)

ggplot(test, aes(x=MSRP, y=MSRP_predictions, color=Diff)) + 
  geom_point() + 
  geom_abline(slope=1, intercept=0, color='red') +
  annotate(geom='text', label=paste('Avg Error -- $', AvgDiff, sep=''), x=400000, y=500000) +
  annotate(geom='text', label=paste('Avg Error Rate -- ', AvgDiffPerc, '%', sep=''),x=400000, y=470000) +
  labs(y='Predicted MSRP', title='Actual vs. Predicted MSRP', x='Actual MSRP') + xlim(0, 500000) + ylim(0, 500000)
```

```{r}
# Using Stepwise Selection - Same Process ---------------------------------------------------------------------------------

# Because Make, Model, & Market Category have many dimensions and lots of variance, we will not include these in the regression
# Also removing Year because Age is more interpretable
data_reg <- data[-c(2, 3, 4, 10)]

# Change Character fields into dummies
char_data_reg <- data_reg %>% select_if(is.character)
char_col_names <- colnames(char_data_reg)
data_reg <- dummy_cols(data_reg, select_columns= char_col_names, remove_selected_columns=TRUE)

# 80/20 Train/Test Split for Objective 1 ----------------------------------------------------------------------------------
idx <- sample(seq(1, 2), size = nrow(data_reg), replace = TRUE, prob = c(.8, .2))
train <- data_reg[idx == 1,]
test <- data_reg[idx == 2,]

# Build Initial Model (Log MSRP) ------------------------------------------------------------------------------------------
model2 <- lm(log(MSRP) ~ .-ID, data=train)
summary(model2)

# Stepwise Selection ------------------------------------------------------------------------------------------------------
model2_step <- stepAIC(model2, direction='both')
summary(model2_step)

# Model2_step Selection ---------------------------------------------------------------------------------------------------
par(mfrow=c(2,2))
plot(model2_step)
ggplot(train, aes(x=model2_step$residuals)) + geom_histogram(fill='red', color='black') + stat_function(fun = dnorm, args = list(mean = mean(model2_step$residuals), sd = sd(model2_step$residuals))) + labs(title='Residual Histogram')

# Making Predictions ------------------------------------------------------------------------------------------------------
test$MSRP_predictions <- predict(model2_step, newdata=test)
test$MSRP_predictions <- exp(test$MSRP_predictions)

test$Diff <- abs(test$MSRP-test$MSRP_predictions)

AvgDiff <- round(mean(test$Diff),2)
AvgDiffPerc <- round(100*(AvgDiff / mean(test$MSRP)),2)

# Plot Differences --------------------------------------------------------------------------------------------------------
ggplot(test, aes(x=MSRP, y=MSRP_predictions, color=Diff)) + 
  geom_point() + 
  geom_abline(slope=1, intercept=0, color='red') +
  annotate(geom='text', label=paste('Avg Error -- $', AvgDiff, sep=''), x=100000, y=500000) +
  annotate(geom='text', label=paste('Avg Error Rate -- %', AvgDiffPerc, sep=''),x=100000, y=470000) +
  labs(y='Predicted MSRP', title='Actual vs. Predicted MSRP', x='Actual MSRP') +
  xlim(0, 600000) + ylim(0, 600000)
```

```{r}
library(glmnet)
# Train, Test, Validate split --------------------------------------------------------------------------------------------
set.seed(1234)
index = sample(seq(1,3),size = nrow(data),replace = TRUE,prob = c(0.8,0.1,0.1))
train = data[index==1,]
test = data[index==2,]
validate = data[index==3,]

#Taking the log of MSRP --------------------------------------------------------------------------------------------------
train = train %>% mutate(log_MSRP = log(MSRP))
test = test %>% mutate(log_MSRP = log(MSRP))
validate = validate %>% mutate(log_MSRP = log(MSRP))

# Split 3 datasets into response and variables
train_response = train[,19]
train_vars = data.matrix(train[,c(5,6,7,8,9,11,12,13,14,16,17,18)])
test_response = test[,19]
test_vars = data.matrix(test[,c(5,6,7,8,9,11,12,13,14,16,17,18)])
validate_response = validate[,19]
validate_vars = data.matrix(validate[,c(5,6,7,8,9,11,12,13,14,16,17,18)])

#Specify the range for lambda
lambdas = 10^seq(3,-2,by=-.1)

#Lasso regression using the response/explanatory variables
lasso_model = glmnet(train_vars,train_response,alpha = 1, lambda = lambdas)
plot(lasso_model)

# Display optimal lambda
optimal_testing_lambda = lasso_model$lambda.min

# Predicting Test set
test_predicted = predict(lasso_model, s=optimal_lambda, newx=test_vars)

# Test MSE for Lasso
testMSE = mean((test_response - test_predicted)^2)
testMSE # 2149

# Define Coefficients
coef(lasso_model, s=optimal_lambda)

# Predict train set
train_predicted = predict(lasso_model, s=optimal_lambda, newx=train_vars)

# Sum of square total and error
sst = sum((train_response - mean(train_response))^2)
sse = sum((train_predicted - train_response)^2)

# R-squared
rsq = 1 - sse/sst
rsq # .8227

# Summary Stats of test regression model
lasso_test_print = lm(log(MSRP) ~ 
                        `Engine Fuel Type` +
                        `Engine HP` +
                        `Transmission Type` +
                        Driven_Wheels +
                        `Number of Doors` +
                        `Vehicle Size` +
                        `Vehicle Style` +
                        `highway MPG` +
                        Popularity +
                        Exotic +
                        Luxury +
                        Age
                        , data=data)
summary(lasso_test_print)
confint(lasso_test_print)

# Validation Lasso Model -------------------------------------------------------------------------------------------------
lasso_cv_model = cv.glmnet(train_vars, train_response, alpha=1, lambda=lambdas)
plot(lasso_cv_model)

# 1se lambda
lasso_cv_model$lambda.1se

# min lambda
lasso_cv_model$lambda.min

# We must compare these two lambdas -- Use Validate set to prepare accuracy statistics

# Predicting Validate Set --- lambda.1se ---------------------------------------------------------------------------------
validate_1se_predicted = predict(lasso_cv_model, s=lasso_cv_model$lambda.1se, newx=validate_vars)

# Test MSE for Lasso
validateMSE_1se = mean((validate_response - validate_1se_predicted)^2)
validateMSE_1se # .2102

# Predict train set
train_predicted_1se = predict(lasso_cv_model, s=lasso_cv_model$lambda.1se, newx=train_vars)

# Sum of square total and error
sst_1se = sum((train_response - mean(train_response))^2)
sse_1se = sum((train_predicted_1se - train_response)^2)

# R-squared
rsq_1se = 1 - sse_1se/sst_1se
rsq_1se # .8191

# Predicting Validate Set --- lambda.min ---------------------------------------------------------------------------------
validate_min_predicted = predict(lasso_cv_model, s=lasso_cv_model$lambda.min, newx=validate_vars)

# Test MSE for Lasso
validateMSE_min = mean((validate_response - validate_min_predicted)^2)
validateMSE_min # .2068

# Predict train set
train_predicted_min = predict(lasso_cv_model, s=lasso_cv_model$lambda.min, newx=train_vars)

# Sum of square total and error
sst_min = sum((train_response - mean(train_response))^2)
sse_min = sum((train_predicted_min - train_response)^2)

# R-squared
rsq_min = 1 - sse_min/sst_min
rsq_min # .8227

# Based on MSE and RSQ, lambda.min is marginally better ------------------------------------------------------------------
coef(lasso_cv_model, lasso_cv_model$lambda.min)

lasso_validate_print = lm(log(MSRP) ~ 
                        `Engine Fuel Type` +
                        `Engine HP` +
                        `Transmission Type` +
                        Driven_Wheels +
                        `Number of Doors` +
                        `Vehicle Size` +
                        `Vehicle Style` +
                        `highway MPG` +
                        Popularity +
                        Exotic +
                        Luxury +
                        Age
                        , data=data)
summary(lasso_validate_print)
confint(lasso_validate_print)
```

```{r}
# Train, Test, Validate Split completed above in Lasso model
train_knn = data.matrix(train[,c(5,6,7,8,9,11,12,13,14,16,17,18, 19)])
test_response = test[,19]
test_vars = data.matrix(test[,c(5,6,7,8,9,11,12,13,14,16,17,18)])
validate_response = validate[,19]
validate_vars = data.matrix(validate[,c(5,6,7,8,9,11,12,13,14,16,17,18)])

# KNN Test Regression ----------------------------------------------------------------------------------------------------
test_model <- train(
  log_MSRP ~ .,
  data = train_knn,
  method = 'knn',
  preProcess = c("center", "scale")
)

# Make Test Predictions
test_predictions = predict(test_model, newdata = test_vars)

# RMSE -- .2212
sqrt(mean((test_response - test_predictions)^2))

# R^2 -- .9611
cor(test_response, test_predictions) ^ 2

test_model$results

# KNN Validate Regression ------------------------------------------------------------------------------------------------
ctrl <- trainControl( method = "cv", number = 10)
tuneGrid <- expand.grid(k = seq(4, 9, by = 1))

val_model <- train(
  log_MSRP ~ .,
  data = train_knn,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid
)
val_model$results
plot(val_model)

# K = 4 is our best option

# Make Test Predictions
validate_predictions = predict(val_model, newdata = validate_vars)

# RMSE -- .1991
sqrt(mean((validate_response - validate_predictions)^2))

# R^2 -- .9644
cor(validate_response, validate_predictions) ^ 2

val_model$results
```